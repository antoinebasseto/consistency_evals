import ast
import json
import os
import re
from typing import Any, Dict, List, Tuple

import numpy as np
import pandas as pd
import plotly.express as px
from InstructorEmbedding import INSTRUCTOR
from scipy.stats import entropy
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

from dataset_handlers.dataset_handler import DatasetHandler


def insert_br_at_whitespace(text, n):
    result = []
    start = 0
    while start < len(text):
        # Look for the nearest whitespace before the nth character
        end = start + n
        if end < len(text) and text[end] != " ":
            # If no whitespace is found, move the end to the nearest space before the nth character
            end = text.rfind(" ", start, end)
            if end == -1:
                end = start + n  # No space found, so split at nth character
        result.append(
            text[start:end].strip()
        )  # Add the chunk, stripping any leading/trailing whitespace
        start = end + 1  # Move to the next chunk, skipping the space
    return "<br>".join(result)


class NEJMHandler(DatasetHandler):
    SYSTEM_PROMPT = {
        "recommendation": "You are a helpful assistant.",
        "hypothesis": "You are a helpful assistant.",
    }
    MESSAGE_PROMPT_START = {
        "recommendation": """
You are a highly knowledgeable and helpful clinical assistant specializing in providing detailed and accurate evaluations of diagnostic hypotheses. Your primary role is to assist clinicians by thoroughly examining and interpreting clinical cases.

You will be provided with a case description and a list of diagnostic hypotheses for this case, generated by the clinician you are assisting.

Your task is to:

    1. Reflect upon the case description, evaluating each hypothesis independently to see if it presents a likely diagnosis for the given case. Be meticulous in evaluating every piece of evidence available in the case description. Ensure each piece of evidence is assessed correctly, offering a balanced view of its implications.
    2. Provide a lead diagnosis from the list of hypotheses provided. This diagnosis should be the most probable based on the evidence presented in the case description.
    3. Provide a rationale explaining why the lead diagnosis you chose is the most probable, citing the case description to support your claims.
    4. Your explanations should be detailed and understandable, intended to assist the clinician in making an informed decision.
    5. Quote direct passages from the case description to support your claims.

Here are the detailed guidelines for completing your task:
    - Thorough Analysis: Take your time to think through each piece of evidence step-by-step. Consider all aspects of the case description and the diagnostic hypotheses.
    - Direct Citations: Any time you use information from the case description, annotate the claim with direct, continuous passages from the text. If multiple passages are needed, cite each one separately WITHOUT using ellipses. Make sure you only cite passages that can be found in the case description! Citations will be collected in the "citations" list, and references to that list made in the "rationale" text, by using square brackets containing the index number of the corresponding citation, e.g. [1].
    - Detailed Explanations: Provide comprehensive explanations that elucidate why the evidence supports the chosen lead diagnosis. Use clear, clinical reasoning to explain your thought process.

Remember, the goal is to recommend the correct lead diagnosis. Your thoroughness and attention to detail are crucial for ensuring the clinician has all the necessary information to make an informed decision.

Take your time to do the task correctly and think things through step by step.

Answer using a JSON format.
""".strip(),
        "hypothesis": """
You are a highly knowledgeable and helpful clinical assistant specializing in providing detailed and accurate evaluations of diagnostic hypotheses. Your primary role is to assist clinicians by thoroughly examining and interpreting clinical cases.

You will be provided with a case description and multiple diagnostic hypotheses for this case, generated by the clinician you are assisting.

Your task is to:

    1. Provide both evidence supporting and contradicting the given hypothesis.
    2. Be meticulous in evaluating every piece of evidence available in the case description.
    3. Ensure each piece of evidence is assessed correctly, offering a balanced view of its implications.
    4. For every claim you make, provide a clear explanation as to why it supports or refutes the hypothesis. Your explanations should be detailed and understandable, intended to assist the clinician in making an informed decision.
    5. Quote direct passages from the case description to support your claims.

Guidelines for completing your task:

    - Direct Citations: Any time you use information from the case description, annotate the claim with direct, continuous passages from the text. If multiple passages are needed, cite each one separately WITHOUT using ellipses. Make sure you only cite passages that can be found in the case description!
    - Thorough Analysis: Take your time to think through each piece of evidence step-by-step. Consider all aspects of the case description and the diagnostic hypothesis.
    - Detailed Explanations: Provide comprehensive explanations that elucidate why the evidence supports or contradicts the hypothesis. Use clear, clinical reasoning to explain your thought process.
    - Balanced Evaluation: Ensure that your analysis is balanced and impartial, presenting a fair evaluation of all evidence.

Remember, the goal is to assist the clinician by providing a clear, well-reasoned analysis of the evidence for and against the diagnostic hypothesis. Your thoroughness and attention to detail are crucial for ensuring the clinician has all the necessary information to make an informed decision.

Take your time to do the task correctly and think things through step by step.

Answer using a JSON format.
""".strip(),
    }

    def __init__(self, engine: str, experiment: str, ai_type: str) -> None:
        super().__init__("nejm", engine, experiment, ai_type)

    def gen_batch_inputs(
        self, k_shot: int = 0, max_options_lists: int = -1, temperature: float = 0.0
    ) -> None:
        if k_shot != 0:
            raise NotImplementedError("k-shot learning is not supported for NEJM")

        for file_name in tqdm(os.listdir(self.get_raw_data_dir())):
            batch_inputs = []
            file_path = os.path.join(self.get_raw_data_dir(), file_name)
            if os.path.isdir(file_path):
                continue
            with open(file_path, "r") as file:
                case_data = json.load(file)
                case_name = file_name.removesuffix(".json")
                case_description = case_data["description"]
                original_options = case_data["hypotheses"]

                options_lists = self._gen_experiment_options_lists(
                    original_options, max_options_lists
                )

                for options in options_lists:
                    prompt = self._gen_prompt(case_description, options)
                    json_schema = (
                        self._gen_json_schema_for_recommendation_driven()
                        if self.ai_type == "recommendation"
                        else self._gen_json_schema_for_hypothesis_driven(options)
                    )
                    batch_input_line = self._gen_batch_input_line(
                        f"{case_name}_{options}",
                        prompt,
                        temperature=temperature,
                        force_json=True,
                        json_schema=json_schema,
                    )
                    batch_inputs.append(batch_input_line)

            with open(
                os.path.join(
                    self.get_batch_inputs_dir(),
                    f"input_{case_name}_temp_{temperature}.jsonl",
                ),
                "w",
            ) as file:
                for entry in batch_inputs:
                    json_line = json.dumps(entry)
                    file.write(json_line + "\n")

    def parse_batch_output_line(self, line: str) -> List[Dict[str, Any]]:
        raw_output = json.loads(line)
        options = ast.literal_eval(raw_output["custom_id"].split("_")[-1])
        raw_answer = raw_output["response"]["body"]["choices"][0]["message"]["content"]
        json_answer = json.loads(raw_answer)

        results = []

        if self.ai_type == "recommendation":
            json_answer["lead_diagnosis"] = re.sub(
                r"^[A-Z]\. ", "", json_answer["lead_diagnosis"]
            )
            result = {
                "options": options,
                "answer": json_answer["lead_diagnosis"],
                "rationale": json_answer["rationale"],
                "citations": json_answer["citations"],
            }
            results.append(result)

        elif self.ai_type == "hypothesis":
            for option in options:
                for evidence_type in ["evidence_for", "evidence_against"]:
                    for claim_and_citations in json_answer[option][evidence_type]:
                        result = {
                            "options": options,
                            "option": option,
                            "evidence_type": evidence_type,
                            "claim": claim_and_citations["claim"],
                            "citations": claim_and_citations["citations"],
                        }
                        results.append(result)

        return results

    def gen_results(self) -> None:
        for file_name in os.listdir(self.get_parsed_batch_outputs_dir()):
            file_path = os.path.join(self.get_parsed_batch_outputs_dir(), file_name)
            df = pd.read_csv(file_path)

            if self.ai_type == "recommendation":
                self._gen_results_for_recommendation_driven(df, file_name)

            if self.ai_type == "hypothesis":
                self._gen_results_for_hypothesis_driven(df, file_name)

    def _gen_results_for_recommendation_driven(
        self, df: pd.DataFrame, file_name: str
    ) -> None:
        results_file_name = file_name.removesuffix(".csv")

        answer_frequencies = df["answer"].value_counts()
        answer_entropy = entropy(answer_frequencies)
        tqdm.write(f"Entropy for {results_file_name}: {answer_entropy}")

        df["answer"] = df["answer"].apply(lambda x: insert_br_at_whitespace(x, 40))
        fig = px.histogram(
            df,
            x="answer",
            category_orders={
                "answer": list(
                    map(
                        lambda x: insert_br_at_whitespace(x, 40),
                        ast.literal_eval(df["options"].iloc[0]),
                    )
                )
            },
        )
        fig.write_html(
            os.path.join(
                self.get_results_dir(), f"{results_file_name}_answer_distribution.html"
            )
        )

    def _gen_results_for_hypothesis_driven(
        self, df: pd.DataFrame, file_name: str
    ) -> None:
        df["answer_id"] = df["options"].factorize()[0].astype(str)
        case_name = "_".join(file_name.split("_")[:2])
        with open(
            os.path.join(self.get_raw_data_dir(), "gold_evidence", f"{case_name}.json")
        ) as file:
            gold = json.load(file)

        if not os.path.exists(
            os.path.join(
                self.get_results_dir(),
                "evidence_matching",
                f"{case_name}_evidence_matching_full.csv",
            )
        ):
            df["matched_gold"] = self._match_evidence(df, gold)
            df.to_csv(
                os.path.join(
                    self.get_results_dir(),
                    "evidence_matching",
                    f"{case_name}_evidence_matching_full.csv",
                ),
                index=False,
            )
        df = pd.read_csv(
            os.path.join(
                self.get_results_dir(),
                "evidence_matching",
                f"{case_name}_evidence_matching_full.csv",
            )
        )
        df["matched_gold"] = df["matched_gold"].apply(ast.literal_eval)

        for option in df["option"].unique():
            for evidence_type in ["evidence_for", "evidence_against"]:
                df_exploded = df[
                    (df["option"] == option) & (df["evidence_type"] == evidence_type)
                ].explode("matched_gold")

                df_exploded["matched_gold"] = df_exploded["matched_gold"].fillna(
                    "other"
                )
                counts = (
                    df_exploded.groupby(["claim", "matched_gold"]).size().reset_index()
                )
                counts.columns = ["claim", "matched_gold", "count"]
                counts_sorted = counts.sort_values(by="count", ascending=False)

                counts_sorted["matched_gold"] = counts_sorted["matched_gold"].apply(
                    lambda x: insert_br_at_whitespace(x, 50)
                )

                fig = px.bar(
                    counts_sorted,
                    x="matched_gold",
                    y="count",
                    hover_data={"claim": True},
                    labels={
                        "matched_gold": "Matched Gold",
                        "count": "Number of Claims",
                    },
                    title=f"{case_name} | {option} | {evidence_type}",
                )
                fig.update_layout(xaxis={"categoryorder": "total descending"})
                fig.write_html(
                    os.path.join(
                        self.get_results_dir(),
                        f"{case_name}_{option}_{evidence_type}_matched_gold_distribution.html",
                    )
                )

        df_exploded = df.explode("matched_gold")
        df_exploded["matched_gold"] = df_exploded["matched_gold"].fillna("other")
        counts = (
            df_exploded.groupby(["option", "evidence_type", "claim", "matched_gold"])
            .size()
            .reset_index()
        )
        counts.columns = ["option", "evidence_type", "claim", "matched_gold", "count"]
        counts_sorted = counts.sort_values(by="count", ascending=False)
        counts_sorted["matched_gold"] = counts_sorted["matched_gold"].apply(
            lambda x: insert_br_at_whitespace(x, 50)
        )
        counts_sorted["has_matched_gold"] = counts_sorted["matched_gold"] != "other"
        fig = px.bar(
            counts_sorted,
            x="evidence_type",
            y="count",
            facet_col="option",
            color="has_matched_gold",
            hover_data={"claim": True, "matched_gold": True},
            labels={"count": "Number of Claims"},
            category_orders={
                "evidence_type": ["evidence_for", "evidence_against"],
                "has_matched_gold": [True, False],
            },
        )
        fig.update_layout(
            showlegend=False,
            width=3840 * 1.5,
            height=1080 * 1.5,
            font=dict(size=36 * 1.5),
            margin=dict(t=240 * 1.5),
        )
        fig.update_xaxes(title=None, tickangle=0, tickfont=dict(size=32 * 1.5))
        fig.for_each_annotation(
            lambda a: a.update(text=insert_br_at_whitespace(a.text.split("=")[-1], 25))
        )
        fig.show()

        results = []
        for option in df["option"].unique():
            for evidence_type in ["evidence_for", "evidence_against"]:
                results.append(
                    {
                        "option": option,
                        "evidence_type": evidence_type,
                        "precision": self._calc_precision(df, option, evidence_type),
                        "recall": self._calc_recall(df, gold, option, evidence_type),
                    }
                )

        results = pd.DataFrame(results)
        print(results)
        print(f"Average precision: {results['precision'].mean()}")
        print(f"Average recall: {results['recall'].mean()}")
        print(f"Case: {case_name}")
        print("*" * 100)

    def _calc_precision(
        self,
        df: pd.DataFrame,
        option: str,
        evidence_type: str,
    ) -> float:
        df_option_type = df[
            (df["option"] == option) & (df["evidence_type"] == evidence_type)
        ]
        true_positives = (
            df_option_type["matched_gold"].apply(lambda x: len(x) > 0).sum()
        )
        false_positives = (
            df_option_type["matched_gold"].apply(lambda x: len(x) == 0).sum()
        )

        return true_positives / (true_positives + false_positives)

    def _calc_recall(
        self,
        df: pd.DataFrame,
        gold: Dict[str, Dict[str, List[str]]],
        option: str,
        evidence_type: str,
    ) -> float:
        gold_option_type = gold[option][evidence_type]
        max_true_positives = len(gold_option_type) * len(df["options"].unique())
        df_option_type = df[
            (df["option"] == option) & (df["evidence_type"] == evidence_type)
        ]
        true_positives = df_option_type["matched_gold"].apply(lambda x: len(x)).sum()
        return true_positives / max_true_positives

    def _match_evidence(
        self, df: pd.DataFrame, gold: Dict[str, Dict[str, List[str]]]
    ) -> pd.Series:
        # model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        # model = SentenceTransformer(
        #     "Alibaba-NLP/gte-large-en-v1.5", trust_remote_code=True
        # )
        # model = SentenceTransformer(
        #     "mixedbread-ai/mxbai-embed-large-v1", truncate_dim=512
        # )
        # model = SentenceTransformer(
        #     "nomic-ai/nomic-embed-text-v1", trust_remote_code=True
        # )
        model = INSTRUCTOR("hkunlp/instructor-large")

        corpus = df["claim"].tolist()
        corpus = [["Represent the Medicine sentence: ", claim] for claim in corpus]
        df["embedding"] = list(model.encode(corpus, show_progress_bar=True))  # type: ignore

        gold_embeddings = {}
        for option in gold.keys():
            gold_embeddings[option] = {}
            for evidence_type in gold[option].keys():
                gold_embeddings[option][evidence_type] = {}
                gold_evidence = gold[option][evidence_type]
                for evidence in gold_evidence:
                    gold_embeddings[option][evidence_type][evidence] = model.encode(
                        [["Represent the Medicine sentence: ", evidence]],  # type: ignore
                        show_progress_bar=True,
                    )

        matches = []

        for _, row in df.iterrows():
            option = row["option"]
            evidence_type = row["evidence_type"]
            gold_evidence_option_type = list(
                gold_embeddings[option][evidence_type].keys()
            )
            gold_embeddings_option_type = list(
                gold_embeddings[option][evidence_type].values()
            )

            similarities = cosine_similarity(
                np.asarray(row["embedding"]).reshape(1, -1),
                np.squeeze(gold_embeddings_option_type, axis=1),  # type: ignore
            )[0]

            good_matches = np.array([])
            index_matches = np.asarray(similarities > 0.9).nonzero()[0]
            if index_matches.size != 0:
                good_matches = np.array(gold_evidence_option_type)[index_matches]

            matches.append(good_matches)

        return pd.Series(matches)

    def _gen_json_schema_for_recommendation_driven(self) -> object:
        return {
            "name": "get_diagnosis",
            "description": "Provide a diagnosis for the given case.",
            "schema": {
                "type": "object",
                "properties": {
                    "rationale": {
                        "type": "string",
                        "description": "Your rationale for the diagnosis you provided, citing the case description to support your claims.",
                    },
                    "citations": {
                        "type": "array",
                        "description": "Direct citation from the case description that back up the rationale",
                        "items": {"type": "string"},
                    },
                    "lead_diagnosis": {
                        "type": "string",
                        "description": "The diagnosis you believe is most likely based on the evidence presented.",
                    },
                },
                "required": ["lead_diagnosis", "citations", "rationale"],
                "additionalProperties": False,
            },
            "strict": True,
        }

    def _gen_json_schema_for_hypothesis_driven(
        self, options: Tuple[str, ...]
    ) -> object:
        schema = {
            "name": "get_evidence_for_and_against",
            "description": "Provide evidence supporting and contradicting the given hypothesis.",
            "schema": {
                "type": "object",
                "properties": {},
                "required": [option for option in options],
                "additionalProperties": False,
            },
            "strict": True,
        }
        for option in options:
            schema["schema"]["properties"][option] = {
                "type": "object",
                "properties": {
                    "evidence_for": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "claim": {
                                    "type": "string",
                                    "description": "A statement presenting evidence supporting the hypothesis, and explaining why.",
                                },
                                "citations": {
                                    "type": "array",
                                    "description": "Direct citation from the case description that back up the claim",
                                    "items": {"type": "string"},
                                },
                            },
                            "required": ["claim", "citations"],
                            "additionalProperties": False,
                        },
                    },
                    "evidence_against": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "claim": {
                                    "type": "string",
                                    "description": "A statement presenting evidence refuting the hypothesis, and explaining why.",
                                },
                                "citations": {
                                    "type": "array",
                                    "description": "Direct citation from the case description that back up the claim",
                                    "items": {"type": "string"},
                                },
                            },
                            "required": ["claim", "citations"],
                            "additionalProperties": False,
                        },
                    },
                },
                "additionalProperties": False,
                "required": ["evidence_for", "evidence_against"],
            }
        return schema
